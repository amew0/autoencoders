1
default cond 1,16_8,8_16,6,24,3
Importing finished!!
cuda is going to be used!!
Dataset loaded!! Length (train dataset) - 24000
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1              [-1, 8, 8, 8]              80
              ReLU-2              [-1, 8, 8, 8]               0
            Conv2d-3             [-1, 16, 6, 6]           1,168
              ReLU-4             [-1, 16, 6, 6]               0
            Conv2d-5             [-1, 24, 3, 3]           3,480
              ReLU-6             [-1, 24, 3, 3]               0
           Flatten-7                  [-1, 216]               0
================================================================
Total params: 4,728
Trainable params: 4,728
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.02
Params size (MB): 0.02
Estimated Total Size (MB): 0.04
----------------------------------------------------------------
None
Task: v2lr, Epoch:1, Loss:0.133454              		Task: InvProb, Epoch:1, Loss:0.034729
Task: v2lr, Epoch:11, Loss:0.135156              		Task: InvProb, Epoch:11, Loss:0.034632
Task: v2lr, Epoch:21, Loss:0.132266              		Task: InvProb, Epoch:21, Loss:0.034210
Task: v2lr, Epoch:31, Loss:0.130582              		Task: InvProb, Epoch:31, Loss:0.034124
Task: v2lr, Epoch:41, Loss:0.129266              		Task: InvProb, Epoch:41, Loss:0.034127
Task: v2lr, Epoch:51, Loss:0.127822              		Task: InvProb, Epoch:51, Loss:0.034029
Task: v2lr, Epoch:61, Loss:0.127159              		Task: InvProb, Epoch:61, Loss:0.033865
Task: v2lr, Epoch:71, Loss:0.127395              		Task: InvProb, Epoch:71, Loss:0.033798
Task: v2lr, Epoch:81, Loss:0.128324              		Task: InvProb, Epoch:81, Loss:0.033742
Task: v2lr, Epoch:91, Loss:0.128407              		Task: InvProb, Epoch:91, Loss:0.033755
Task: v2lr, Epoch:101, Loss:0.128483              		Task: InvProb, Epoch:101, Loss:0.033758
Task: v2lr, Epoch:111, Loss:0.128209              		Task: InvProb, Epoch:111, Loss:0.033883
Task: v2lr, Epoch:121, Loss:0.128574              		Task: InvProb, Epoch:121, Loss:0.033902
Task: v2lr, Epoch:131, Loss:0.128842              		Task: InvProb, Epoch:131, Loss:0.033934
Task: v2lr, Epoch:141, Loss:0.128801              		Task: InvProb, Epoch:141, Loss:0.033952
Task: v2lr, Epoch:151, Loss:0.128593              		Task: InvProb, Epoch:151, Loss:0.033949
Task: v2lr, Epoch:161, Loss:0.128624              		Task: InvProb, Epoch:161, Loss:0.033930
Task: v2lr, Epoch:171, Loss:0.128335              		Task: InvProb, Epoch:171, Loss:0.033856
Task: v2lr, Epoch:181, Loss:0.128686              		Task: InvProb, Epoch:181, Loss:0.033896
Task: v2lr, Epoch:191, Loss:0.128226              		Task: InvProb, Epoch:191, Loss:0.033910
Task: v2lr, Epoch:200, Loss:0.128262              		Task: InvProb, Epoch:200, Loss:0.033888
Avg Test Loss: 0.22663854374488196
Max Test Loss: 0.6736299991607666
Min Test Loss: 0.03448403999209404
written to: ./results/loss_tracker_v2lr.csv
written to: ./models/v2lr/1.20231117012555_v2lr.pth
written to: ./models/v2lr/1.20231117012555_v2lr.pt
Elapsed time: 852.3636028766632 seconds.
