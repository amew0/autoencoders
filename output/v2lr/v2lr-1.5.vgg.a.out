2692860
1.5.vgg.a
dataloading with npf=True
Importing finished!!
cuda is going to be used!!
Dataset loaded!! Length (train dataset) - 19200
/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 6, 12, 12]             156
              ReLU-2            [-1, 6, 12, 12]               0
            Conv2d-3            [-1, 6, 12, 12]             330
            Conv2d-4            [-1, 6, 16, 16]              12
              ReLU-5            [-1, 6, 12, 12]               0
     ResidualBlock-6            [-1, 6, 12, 12]               0
       BatchNorm2d-7            [-1, 6, 12, 12]              12
            Conv2d-8             [-1, 18, 8, 8]           2,718
              ReLU-9             [-1, 18, 8, 8]               0
           Conv2d-10             [-1, 18, 8, 8]           2,934
           Conv2d-11           [-1, 18, 12, 12]             126
             ReLU-12             [-1, 18, 8, 8]               0
    ResidualBlock-13             [-1, 18, 8, 8]               0
      BatchNorm2d-14             [-1, 18, 8, 8]              36
           Conv2d-15             [-1, 54, 4, 4]          24,354
             ReLU-16             [-1, 54, 4, 4]               0
           Conv2d-17             [-1, 54, 4, 4]          26,298
           Conv2d-18             [-1, 54, 8, 8]           1,026
             ReLU-19             [-1, 54, 4, 4]               0
    ResidualBlock-20             [-1, 54, 4, 4]               0
      BatchNorm2d-21             [-1, 54, 4, 4]             108
           Conv2d-22            [-1, 108, 2, 2]          52,596
             ReLU-23            [-1, 108, 2, 2]               0
           Conv2d-24            [-1, 108, 2, 2]         105,084
           Conv2d-25            [-1, 108, 4, 4]           5,940
             ReLU-26            [-1, 108, 2, 2]               0
    ResidualBlock-27            [-1, 108, 2, 2]               0
      BatchNorm2d-28            [-1, 108, 2, 2]             216
           Conv2d-29            [-1, 216, 1, 1]          93,528
             ReLU-30            [-1, 216, 1, 1]               0
           Conv2d-31            [-1, 216, 1, 1]         420,120
           Conv2d-32            [-1, 216, 2, 2]          23,544
             ReLU-33            [-1, 216, 1, 1]               0
    ResidualBlock-34            [-1, 216, 1, 1]               0
      BatchNorm2d-35            [-1, 216, 1, 1]             432
          Flatten-36                  [-1, 216]               0
================================================================
Total params: 759,570
Trainable params: 759,570
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.24
Params size (MB): 2.90
Estimated Total Size (MB): 3.14
----------------------------------------------------------------
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         Unflatten-1             [-1, 24, 3, 3]               0
   ConvTranspose2d-2            [-1, 192, 6, 6]          41,664
              ReLU-3            [-1, 192, 6, 6]               0
   ConvTranspose2d-4            [-1, 192, 6, 6]         331,968
   ConvTranspose2d-5            [-1, 192, 3, 3]           4,800
              ReLU-6            [-1, 192, 6, 6]               0
    ResidualBlockk-7            [-1, 192, 6, 6]               0
       BatchNorm2d-8            [-1, 192, 6, 6]             384
   ConvTranspose2d-9             [-1, 96, 8, 8]         165,984
             ReLU-10             [-1, 96, 8, 8]               0
  ConvTranspose2d-11             [-1, 96, 8, 8]          83,040
  ConvTranspose2d-12             [-1, 96, 6, 6]          18,528
             ReLU-13             [-1, 96, 8, 8]               0
   ResidualBlockk-14             [-1, 96, 8, 8]               0
      BatchNorm2d-15             [-1, 96, 8, 8]             192
  ConvTranspose2d-16           [-1, 48, 12, 12]          18,480
             ReLU-17           [-1, 48, 12, 12]               0
  ConvTranspose2d-18           [-1, 48, 12, 12]          20,784
  ConvTranspose2d-19             [-1, 48, 8, 8]           4,656
             ReLU-20           [-1, 48, 12, 12]               0
   ResidualBlockk-21           [-1, 48, 12, 12]               0
      BatchNorm2d-22           [-1, 48, 12, 12]              96
  ConvTranspose2d-23            [-1, 1, 24, 24]             433
             ReLU-24            [-1, 1, 24, 24]               0
  ConvTranspose2d-25            [-1, 1, 24, 24]              10
  ConvTranspose2d-26            [-1, 1, 12, 12]              49
             ReLU-27            [-1, 1, 24, 24]               0
   ResidualBlockk-28            [-1, 1, 24, 24]               0
      BatchNorm2d-29            [-1, 1, 24, 24]               2
================================================================
Total params: 691,070
Trainable params: 691,070
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 1.01
Params size (MB): 2.64
Estimated Total Size (MB): 3.64
----------------------------------------------------------------
Ready to TRAIN!!
Task: Training Epoch @ 000 L: 1.968372 M: 0.555498 S: 0.849784  -- V2LR: Epoch M: 0.779428 !==! Task: Validation Epoch @ 000 L: 1.623043 M: 0.649669 S: 0.977180  -- V2LR: Epoch M: 38.863644
Task: Training Epoch @ 001 L: 1.559735 M: 0.579254 S: 0.981164  -- V2LR: Epoch M: 0.656233 !==! Task: Validation Epoch @ 001 L: 1.552885 M: 0.589123 S: 0.980340  -- V2LR: Epoch M: 9.853083
Task: Training Epoch @ 002 L: 1.537300 M: 0.570448 S: 0.979385  -- V2LR: Epoch M: 0.422023 !==! Task: Validation Epoch @ 002 L: 1.531271 M: 0.555695 S: 0.977864  -- V2LR: Epoch M: 6.665974
Task: Training Epoch @ 003 L: 1.522550 M: 0.547348 S: 0.976751  -- V2LR: Epoch M: 0.235819 !==! Task: Validation Epoch @ 003 L: 1.502293 M: 0.529119 S: 0.975040  -- V2LR: Epoch M: 0.193205
Task: Training Epoch @ 004 L: 1.506513 M: 0.517706 S: 0.973776  -- V2LR: Epoch M: 0.168566 !==! Task: Validation Epoch @ 004 L: 1.481276 M: 0.502778 S: 0.971635  -- V2LR: Epoch M: 0.191334
Task: Training Epoch @ 005 L: 1.491601 M: 0.491234 S: 0.970740  -- V2LR: Epoch M: 0.142407 !==! Task: Validation Epoch @ 005 L: 1.475714 M: 0.478389 S: 0.968946  -- V2LR: Epoch M: 0.145502
Task: Training Epoch @ 006 L: 1.481209 M: 0.462442 S: 0.968043  -- V2LR: Epoch M: 0.124479 !==! Task: Validation Epoch @ 006 L: 1.461003 M: 0.439314 S: 0.965459  -- V2LR: Epoch M: 0.122248
Task: Training Epoch @ 008 L: 1.463172 M: 0.418557 S: 0.963724  -- V2LR: Epoch M: 0.106323 !==! Task: Validation Epoch @ 008 L: 1.440376 M: 0.408602 S: 0.959937  -- V2LR: Epoch M: 0.102485
Task: Training Epoch @ 009 L: 1.451152 M: 0.390910 S: 0.960639  -- V2LR: Epoch M: 0.102461 !==! Task: Validation Epoch @ 009 L: 1.471851 M: 0.401481 S: 0.957746  -- V2LR: Epoch M: 0.104807
Task: Training Epoch @ 010 L: 1.444344 M: 0.369233 S: 0.958262  -- V2LR: Epoch M: 0.098831 !==! Task: Validation Epoch @ 010 L: 1.445747 M: 0.346187 S: 0.952701  -- V2LR: Epoch M: 0.117763
Task: Training Epoch @ 011 L: 1.428417 M: 0.300071 S: 0.939443  -- V2LR: Epoch M: 0.098218 !==! Task: Validation Epoch @ 011 L: 1.434486 M: 0.263530 S: 0.916529  -- V2LR: Epoch M: 0.099242
Task: Training Epoch @ 012 L: 1.410042 M: 0.269062 S: 0.881084  -- V2LR: Epoch M: 0.096447 !==! Task: Validation Epoch @ 012 L: 1.411873 M: 0.244429 S: 0.873641  -- V2LR: Epoch M: 0.106200
Task: Training Epoch @ 017 L: 1.365323 M: 0.252041 S: 0.825735  -- V2LR: Epoch M: 0.098156 !==! Task: Validation Epoch @ 017 L: 1.431336 M: 0.237891 S: 0.836341  -- V2LR: Epoch M: 0.090625
Task: Training Epoch @ 027 L: 1.284971 M: 0.249195 S: 0.778772  -- V2LR: Epoch M: 0.106684 !==! Task: Validation Epoch @ 027 L: 1.410606 M: 0.221378 S: 0.765418  -- V2LR: Epoch M: 0.100192
Task: Training Epoch @ 033 L: 1.239261 M: 0.259274 S: 0.761850  -- V2LR: Epoch M: 0.113238 !==! Task: Validation Epoch @ 033 L: 1.396450 M: 0.219701 S: 0.721118  -- V2LR: Epoch M: 0.111858
Task: Training Epoch @ 047 L: 1.166080 M: 0.263794 S: 0.731962  -- V2LR: Epoch M: 0.129244 !==! Task: Validation Epoch @ 047 L: 1.394276 M: 0.194757 S: 0.772159  -- V2LR: Epoch M: 0.122358
Tolerance: 3!! Task: Training Epoch @ 068 L: 1.110109 M: 0.257148 S: 0.710577  -- V2LR: Epoch M: 0.150655 !==! Task: Validation Epoch @ 068 L: 1.744126 M: 0.363968 S: 0.673719  -- V2LR: Epoch M: 0.155596
Tolerance: 2!! Task: Training Epoch @ 089 L: 1.118932 M: 0.239891 S: 0.709174  -- V2LR: Epoch M: 0.166001 !==! Task: Validation Epoch @ 089 L: 1.467036 M: 0.248046 S: 0.735866  -- V2LR: Epoch M: 0.155771
Tolerance: 1!! Task: Training Epoch @ 110 L: 1.028783 M: 0.233597 S: 0.654837  -- V2LR: Epoch M: 0.178771 !==! Task: Validation Epoch @ 110 L: 1.385522 M: 0.208779 S: 0.697895  -- V2LR: Epoch M: 0.171229
Task: Testing Epoch @ -01 L: 1.585988 M: 0.206034 S: 0.796926  -- V2LR: Epoch M: 0.123415
written to: ./models/v2lr/1.5.vgg.a.20231221000032_v2lr.pt
written to: ./results/loss_tracker_v2lr.csv
Elapsed time: 4442.68959903717 seconds.
