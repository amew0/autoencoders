2611187
1.2.5
from the freshly retrained ./models/img/14.2.1.retraining.2.20231130014311_img.pt
Importing finished!!
cuda is going to be used!!
Dataset loaded!! Length (train dataset) - 24000
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1              [-1, 8, 8, 8]              80
       BatchNorm2d-2              [-1, 8, 8, 8]              16
            Conv2d-3              [-1, 8, 8, 8]             584
              ReLU-4              [-1, 8, 8, 8]               0
            Conv2d-5             [-1, 16, 6, 6]           1,168
       BatchNorm2d-6             [-1, 16, 6, 6]              32
            Conv2d-7             [-1, 16, 6, 6]           2,320
              ReLU-8             [-1, 16, 6, 6]               0
            Conv2d-9             [-1, 24, 3, 3]           3,480
      BatchNorm2d-10             [-1, 24, 3, 3]              48
           Conv2d-11             [-1, 24, 3, 3]           5,208
             ReLU-12             [-1, 24, 3, 3]               0
          Flatten-13                  [-1, 216]               0
================================================================
Total params: 12,936
Trainable params: 12,936
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.04
Params size (MB): 0.05
Estimated Total Size (MB): 0.09
----------------------------------------------------------------
None
Reconstructor from: ./models/img/14.2.1.retraining.2.20231130014311_img.pt
Task: v2lr, Epoch:1, Epoch Loss: 0.051253341138362885, Loss:0.053675, MSELoss:0.004685                	Task: InvProb, Epoch:1, MSELoss:0.370515, SSIM:1.0266472731884375
Best state dict, with mse_loss 0.004685, yet found @ 0...
Best state dict, with mse_loss 0.004418, yet found @ 1...
Best state dict, with mse_loss 0.004405, yet found @ 2...
Best state dict, with mse_loss 0.004342, yet found @ 3...
Best state dict, with mse_loss 0.004267, yet found @ 4...
Task: v2lr, Epoch:11, Epoch Loss: 0.04793397709727287, Loss:0.053886, MSELoss:0.004426                	Task: InvProb, Epoch:11, MSELoss:0.372683, SSIM:1.0334482942638685
Best state dict, with mse_loss 0.004238, yet found @ 16...
Task: v2lr, Epoch:21, Epoch Loss: 0.04791976884007454, Loss:0.054247, MSELoss:0.004423                	Task: InvProb, Epoch:21, MSELoss:0.363968, SSIM:1.0407127969416958
Task: v2lr, Epoch:31, Epoch Loss: 0.047920096665620804, Loss:0.053870, MSELoss:0.004424                	Task: InvProb, Epoch:31, MSELoss:0.368675, SSIM:1.033156293407204
Task: v2lr, Epoch:41, Epoch Loss: 0.047906819730997086, Loss:0.053937, MSELoss:0.004363                	Task: InvProb, Epoch:41, MSELoss:0.376604, SSIM:1.0351057188230668
Task: v2lr, Epoch:51, Epoch Loss: 0.047932639718055725, Loss:0.053875, MSELoss:0.004309                	Task: InvProb, Epoch:51, MSELoss:0.362359, SSIM:1.0344120885332257
Task: v2lr, Epoch:61, Epoch Loss: 0.04792000725865364, Loss:0.052880, MSELoss:0.004292                	Task: InvProb, Epoch:61, MSELoss:0.376105, SSIM:1.0146775773461614
Best state dict, with mse_loss 0.004232, yet found @ 64...
Task: v2lr, Epoch:71, Epoch Loss: 0.04786728322505951, Loss:0.053311, MSELoss:0.004454                	Task: InvProb, Epoch:71, MSELoss:0.373838, SSIM:1.0216860636690908
Task: v2lr, Epoch:81, Epoch Loss: 0.047868601977825165, Loss:0.051997, MSELoss:0.004294                	Task: InvProb, Epoch:81, MSELoss:0.322319, SSIM:0.9970042052234194
Best state dict, with mse_loss 0.004193, yet found @ 87...
Task: v2lr, Epoch:91, Epoch Loss: 0.04791079834103584, Loss:0.052219, MSELoss:0.004264                	Task: InvProb, Epoch:91, MSELoss:0.269952, SSIM:1.0017452983413007
Task: v2lr, Epoch:101, Epoch Loss: 0.047890692949295044, Loss:0.052969, MSELoss:0.004333                	Task: InvProb, Epoch:101, MSELoss:0.357348, SSIM:1.0160506241834486
Task: v2lr, Epoch:111, Epoch Loss: 0.04785195738077164, Loss:0.052889, MSELoss:0.004340                	Task: InvProb, Epoch:111, MSELoss:0.351495, SSIM:1.0143824252587563
Best state dict, with mse_loss 0.004162, yet found @ 115...
Task: v2lr, Epoch:121, Epoch Loss: 0.04784800484776497, Loss:0.052925, MSELoss:0.004300                	Task: InvProb, Epoch:121, MSELoss:0.344272, SSIM:1.0155071300135559
Task: v2lr, Epoch:131, Epoch Loss: 0.047848355025053024, Loss:0.053402, MSELoss:0.004248                	Task: InvProb, Epoch:131, MSELoss:0.334784, SSIM:1.0255545702873574
Task: v2lr, Epoch:141, Epoch Loss: 0.04784749820828438, Loss:0.053195, MSELoss:0.004213                	Task: InvProb, Epoch:141, MSELoss:0.319698, SSIM:1.0217583758010522
Task: v2lr, Epoch:151, Epoch Loss: 0.04789222776889801, Loss:0.053170, MSELoss:0.004301                	Task: InvProb, Epoch:151, MSELoss:0.325992, SSIM:1.0203897116894078
Task: v2lr, Epoch:161, Epoch Loss: 0.04785546660423279, Loss:0.052571, MSELoss:0.004397                	Task: InvProb, Epoch:161, MSELoss:0.325934, SSIM:1.0074578733064834
Task: v2lr, Epoch:171, Epoch Loss: 0.04788143187761307, Loss:0.053110, MSELoss:0.004207                	Task: InvProb, Epoch:171, MSELoss:0.357691, SSIM:1.0201317189406693
Task: v2lr, Epoch:181, Epoch Loss: 0.04786600545048714, Loss:0.053487, MSELoss:0.004259                	Task: InvProb, Epoch:181, MSELoss:0.344886, SSIM:1.0271462124534683
Task: v2lr, Epoch:191, Epoch Loss: 0.047876764088869095, Loss:0.053311, MSELoss:0.004225                	Task: InvProb, Epoch:191, MSELoss:0.347822, SSIM:1.0239747030403026
Task: v2lr, Epoch:200, Epoch Loss: 0.04780671373009682, Loss:0.053905, MSELoss:0.004339                	Task: InvProb, Epoch:200, MSELoss:0.344976, SSIM:1.0347058919303305
Traceback (most recent call last):
  File "/home/kunet.ae/100053678/eit/autoencoders/v2lr.py", line 243, in <module>
    batch_encoded = recon.encoder[1:](batch_img)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Given groups=1, weight of size [96, 48, 2, 2], expected input[4, 1, 24, 24] to have 48 channels, but got 1 channels instead
