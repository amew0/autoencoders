0
6 cond 1 linear layer
Importing finished!!
cuda is going to be used!!
Dataset loaded!! Length (train dataset) - 20160
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 256]               0
            Linear-2                  [-1, 216]          55,512
================================================================
Total params: 55,512
Trainable params: 55,512
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.21
Estimated Total Size (MB): 0.22
----------------------------------------------------------------
None
Task: v2lr, Epoch:1, Loss:0.022478              		Task: InvProb, Epoch:1, Loss:0.004931
Task: v2lr, Epoch:11, Loss:0.021480              		Task: InvProb, Epoch:11, Loss:0.004851
Task: v2lr, Epoch:21, Loss:0.021430              		Task: InvProb, Epoch:21, Loss:0.004844
Task: v2lr, Epoch:31, Loss:0.021419              		Task: InvProb, Epoch:31, Loss:0.004843
Task: v2lr, Epoch:41, Loss:0.021416              		Task: InvProb, Epoch:41, Loss:0.004842
Task: v2lr, Epoch:51, Loss:0.021414              		Task: InvProb, Epoch:51, Loss:0.004842
Task: v2lr, Epoch:61, Loss:0.021414              		Task: InvProb, Epoch:61, Loss:0.004842
Task: v2lr, Epoch:71, Loss:0.021413              		Task: InvProb, Epoch:71, Loss:0.004842
Task: v2lr, Epoch:81, Loss:0.021413              		Task: InvProb, Epoch:81, Loss:0.004842
Task: v2lr, Epoch:91, Loss:0.021413              		Task: InvProb, Epoch:91, Loss:0.004842
Task: v2lr, Epoch:101, Loss:0.021413              		Task: InvProb, Epoch:101, Loss:0.004842
Task: v2lr, Epoch:111, Loss:0.021413              		Task: InvProb, Epoch:111, Loss:0.004842
Task: v2lr, Epoch:121, Loss:0.021413              		Task: InvProb, Epoch:121, Loss:0.004842
Task: v2lr, Epoch:131, Loss:0.021413              		Task: InvProb, Epoch:131, Loss:0.004842
Task: v2lr, Epoch:141, Loss:0.021413              		Task: InvProb, Epoch:141, Loss:0.004842
Task: v2lr, Epoch:151, Loss:0.021413              		Task: InvProb, Epoch:151, Loss:0.004842
Task: v2lr, Epoch:161, Loss:0.021413              		Task: InvProb, Epoch:161, Loss:0.004842
Task: v2lr, Epoch:171, Loss:0.021413              		Task: InvProb, Epoch:171, Loss:0.004842
Task: v2lr, Epoch:181, Loss:0.021413              		Task: InvProb, Epoch:181, Loss:0.004842
Task: v2lr, Epoch:191, Loss:0.021413              		Task: InvProb, Epoch:191, Loss:0.004842
Task: v2lr, Epoch:200, Loss:0.021413              		Task: InvProb, Epoch:200, Loss:0.004842
Avg Test Loss: 0.014566232531612355
Max Test Loss: 0.04171430692076683
Min Test Loss: 0.003837809432297945
written to: ./results/loss_tracker_v2lr.csv
written to: ./models/v2lr/6_0.20231116213944_v2lr.pth
written to: ./models/v2lr/6_0.20231116213944_v2lr.pt
Elapsed time: 606.2578408718109 seconds.
