0
1 linear layer
Importing finished!!
cuda is going to be used!!
Dataset loaded!! Length (train dataset) - 24000
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 256]               0
            Linear-2                  [-1, 216]          55,512
================================================================
Total params: 55,512
Trainable params: 55,512
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.21
Estimated Total Size (MB): 0.22
----------------------------------------------------------------
None
Task: v2lr, Epoch:1, Loss:0.113919              		Task: InvProb, Epoch:1, Loss:0.033652
Task: v2lr, Epoch:11, Loss:0.111084              		Task: InvProb, Epoch:11, Loss:0.032623
Task: v2lr, Epoch:21, Loss:0.110852              		Task: InvProb, Epoch:21, Loss:0.032587
Task: v2lr, Epoch:31, Loss:0.110789              		Task: InvProb, Epoch:31, Loss:0.032577
Task: v2lr, Epoch:41, Loss:0.110766              		Task: InvProb, Epoch:41, Loss:0.032573
Task: v2lr, Epoch:51, Loss:0.110760              		Task: InvProb, Epoch:51, Loss:0.032574
Task: v2lr, Epoch:61, Loss:0.110758              		Task: InvProb, Epoch:61, Loss:0.032576
Task: v2lr, Epoch:71, Loss:0.110757              		Task: InvProb, Epoch:71, Loss:0.032576
Task: v2lr, Epoch:81, Loss:0.110755              		Task: InvProb, Epoch:81, Loss:0.032576
Task: v2lr, Epoch:91, Loss:0.110755              		Task: InvProb, Epoch:91, Loss:0.032575
Task: v2lr, Epoch:101, Loss:0.110754              		Task: InvProb, Epoch:101, Loss:0.032575
Task: v2lr, Epoch:111, Loss:0.110754              		Task: InvProb, Epoch:111, Loss:0.032575
Task: v2lr, Epoch:121, Loss:0.110754              		Task: InvProb, Epoch:121, Loss:0.032575
Task: v2lr, Epoch:131, Loss:0.110753              		Task: InvProb, Epoch:131, Loss:0.032575
Task: v2lr, Epoch:141, Loss:0.110753              		Task: InvProb, Epoch:141, Loss:0.032575
Task: v2lr, Epoch:151, Loss:0.110753              		Task: InvProb, Epoch:151, Loss:0.032575
Task: v2lr, Epoch:161, Loss:0.110753              		Task: InvProb, Epoch:161, Loss:0.032575
Task: v2lr, Epoch:171, Loss:0.110753              		Task: InvProb, Epoch:171, Loss:0.032575
Task: v2lr, Epoch:181, Loss:0.110753              		Task: InvProb, Epoch:181, Loss:0.032575
Task: v2lr, Epoch:191, Loss:0.110753              		Task: InvProb, Epoch:191, Loss:0.032575
Task: v2lr, Epoch:200, Loss:0.110466              		Task: InvProb, Epoch:200, Loss:0.032743
Avg Test Loss: 0.21523680334289869
written to: ./results/loss_tracker_v2lr.csv
written to: ./models/v2lr/0.20231116212748_v2lr.pth
written to: ./models/v2lr/0.20231116212748_v2lr.pt
Elapsed time: 710.9918491840363 seconds.
