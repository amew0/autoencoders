2693120
1.5.vgg.b.1
base 1.5.vgg dataloading with npf=True loss=vggloss+10mse
Importing finished!!
cuda is going to be used!!
Dataset loaded!! Length (train dataset) - 19200
/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/kunet.ae/100053678/.conda/envs/eit/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 6, 12, 12]             156
              ReLU-2            [-1, 6, 12, 12]               0
            Conv2d-3            [-1, 6, 12, 12]             330
            Conv2d-4            [-1, 6, 16, 16]              12
              ReLU-5            [-1, 6, 12, 12]               0
     ResidualBlock-6            [-1, 6, 12, 12]               0
       BatchNorm2d-7            [-1, 6, 12, 12]              12
            Conv2d-8             [-1, 18, 8, 8]           2,718
              ReLU-9             [-1, 18, 8, 8]               0
           Conv2d-10             [-1, 18, 8, 8]           2,934
           Conv2d-11           [-1, 18, 12, 12]             126
             ReLU-12             [-1, 18, 8, 8]               0
    ResidualBlock-13             [-1, 18, 8, 8]               0
      BatchNorm2d-14             [-1, 18, 8, 8]              36
           Conv2d-15             [-1, 54, 4, 4]          24,354
             ReLU-16             [-1, 54, 4, 4]               0
           Conv2d-17             [-1, 54, 4, 4]          26,298
           Conv2d-18             [-1, 54, 8, 8]           1,026
             ReLU-19             [-1, 54, 4, 4]               0
    ResidualBlock-20             [-1, 54, 4, 4]               0
      BatchNorm2d-21             [-1, 54, 4, 4]             108
           Conv2d-22            [-1, 108, 2, 2]          52,596
             ReLU-23            [-1, 108, 2, 2]               0
           Conv2d-24            [-1, 108, 2, 2]         105,084
           Conv2d-25            [-1, 108, 4, 4]           5,940
             ReLU-26            [-1, 108, 2, 2]               0
    ResidualBlock-27            [-1, 108, 2, 2]               0
      BatchNorm2d-28            [-1, 108, 2, 2]             216
           Conv2d-29            [-1, 216, 1, 1]          93,528
             ReLU-30            [-1, 216, 1, 1]               0
           Conv2d-31            [-1, 216, 1, 1]         420,120
           Conv2d-32            [-1, 216, 2, 2]          23,544
             ReLU-33            [-1, 216, 1, 1]               0
    ResidualBlock-34            [-1, 216, 1, 1]               0
      BatchNorm2d-35            [-1, 216, 1, 1]             432
          Flatten-36                  [-1, 216]               0
================================================================
Total params: 759,570
Trainable params: 759,570
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.24
Params size (MB): 2.90
Estimated Total Size (MB): 3.14
----------------------------------------------------------------
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         Unflatten-1             [-1, 24, 3, 3]               0
   ConvTranspose2d-2            [-1, 192, 6, 6]          41,664
              ReLU-3            [-1, 192, 6, 6]               0
   ConvTranspose2d-4            [-1, 192, 6, 6]         331,968
   ConvTranspose2d-5            [-1, 192, 3, 3]           4,800
              ReLU-6            [-1, 192, 6, 6]               0
    ResidualBlockk-7            [-1, 192, 6, 6]               0
       BatchNorm2d-8            [-1, 192, 6, 6]             384
   ConvTranspose2d-9             [-1, 96, 8, 8]         165,984
             ReLU-10             [-1, 96, 8, 8]               0
  ConvTranspose2d-11             [-1, 96, 8, 8]          83,040
  ConvTranspose2d-12             [-1, 96, 6, 6]          18,528
             ReLU-13             [-1, 96, 8, 8]               0
   ResidualBlockk-14             [-1, 96, 8, 8]               0
      BatchNorm2d-15             [-1, 96, 8, 8]             192
  ConvTranspose2d-16           [-1, 48, 12, 12]          18,480
             ReLU-17           [-1, 48, 12, 12]               0
  ConvTranspose2d-18           [-1, 48, 12, 12]          20,784
  ConvTranspose2d-19             [-1, 48, 8, 8]           4,656
             ReLU-20           [-1, 48, 12, 12]               0
   ResidualBlockk-21           [-1, 48, 12, 12]               0
      BatchNorm2d-22           [-1, 48, 12, 12]              96
  ConvTranspose2d-23            [-1, 1, 24, 24]             433
             ReLU-24            [-1, 1, 24, 24]               0
  ConvTranspose2d-25            [-1, 1, 24, 24]              10
  ConvTranspose2d-26            [-1, 1, 12, 12]              49
             ReLU-27            [-1, 1, 24, 24]               0
   ResidualBlockk-28            [-1, 1, 24, 24]               0
      BatchNorm2d-29            [-1, 1, 24, 24]               2
================================================================
Total params: 691,070
Trainable params: 691,070
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 1.01
Params size (MB): 2.64
Estimated Total Size (MB): 3.64
----------------------------------------------------------------
Ready to TRAIN!!
Task: Training Epoch @ 000 L: 2.217244 M: 0.082023 S: 0.395251  -- V2LR: Epoch M: 0.105055 !==! Task: Validation Epoch @ 000 L: 2.098159 M: 0.073879 S: 0.369820  -- V2LR: Epoch M: 0.106511
Task: Training Epoch @ 001 L: 2.147660 M: 0.077632 S: 0.366604  -- V2LR: Epoch M: 0.106072 !==! Task: Validation Epoch @ 001 L: 2.082059 M: 0.073222 S: 0.356702  -- V2LR: Epoch M: 0.109840
Task: Training Epoch @ 002 L: 2.107604 M: 0.075582 S: 0.356455  -- V2LR: Epoch M: 0.107271 !==! Task: Validation Epoch @ 002 L: 2.067916 M: 0.072602 S: 0.341814  -- V2LR: Epoch M: 0.109562
Tolerance: 3!! Task: Training Epoch @ 023 L: 1.505205 M: 0.046444 S: 0.228933  -- V2LR: Epoch M: 0.127066 !==! Task: Validation Epoch @ 023 L: 2.136352 M: 0.079866 S: 0.306238  -- V2LR: Epoch M: 0.131724
Task: Training Epoch @ 042 L: 1.275545 M: 0.037238 S: 0.193011  -- V2LR: Epoch M: 0.139241 !==! Task: Validation Epoch @ 042 L: 2.002139 M: 0.072261 S: 0.299845  -- V2LR: Epoch M: 0.140152
Task: Training Epoch @ 048 L: 1.226200 M: 0.035365 S: 0.185478  -- V2LR: Epoch M: 0.142888 !==! Task: Validation Epoch @ 048 L: 2.001342 M: 0.071916 S: 0.295740  -- V2LR: Epoch M: 0.143163
Task: Training Epoch @ 056 L: 1.204851 M: 0.034651 S: 0.184189  -- V2LR: Epoch M: 0.148054 !==! Task: Validation Epoch @ 056 L: 1.975073 M: 0.071188 S: 0.300651  -- V2LR: Epoch M: 0.147297
Task: Training Epoch @ 058 L: 1.168319 M: 0.033215 S: 0.177608  -- V2LR: Epoch M: 0.148761 !==! Task: Validation Epoch @ 058 L: 1.986816 M: 0.071128 S: 0.292932  -- V2LR: Epoch M: 0.148198
Task: Training Epoch @ 059 L: 1.163202 M: 0.033025 S: 0.177603  -- V2LR: Epoch M: 0.149535 !==! Task: Validation Epoch @ 059 L: 1.973719 M: 0.070276 S: 0.294941  -- V2LR: Epoch M: 0.148408
Task: Training Epoch @ 062 L: 1.141960 M: 0.032261 S: 0.174068  -- V2LR: Epoch M: 0.151338 !==! Task: Validation Epoch @ 062 L: 1.969745 M: 0.069993 S: 0.292683  -- V2LR: Epoch M: 0.151019
Task: Training Epoch @ 069 L: 1.109049 M: 0.031088 S: 0.170302  -- V2LR: Epoch M: 0.155602 !==! Task: Validation Epoch @ 069 L: 1.956077 M: 0.069727 S: 0.292046  -- V2LR: Epoch M: 0.154963
Task: Training Epoch @ 070 L: 1.100142 M: 0.030726 S: 0.168232  -- V2LR: Epoch M: 0.156152 !==! Task: Validation Epoch @ 070 L: 1.941184 M: 0.068888 S: 0.291189  -- V2LR: Epoch M: 0.152748
Task: Training Epoch @ 083 L: 1.050014 M: 0.029066 S: 0.161862  -- V2LR: Epoch M: 0.163465 !==! Task: Validation Epoch @ 083 L: 1.918957 M: 0.067816 S: 0.293433  -- V2LR: Epoch M: 0.158090
Task: Training Epoch @ 090 L: 1.026663 M: 0.028304 S: 0.158950  -- V2LR: Epoch M: 0.167295 !==! Task: Validation Epoch @ 090 L: 1.908935 M: 0.067384 S: 0.291672  -- V2LR: Epoch M: 0.165695
Tolerance: 2!! Task: Training Epoch @ 111 L: 0.974549 M: 0.026706 S: 0.151875  -- V2LR: Epoch M: 0.178771 !==! Task: Validation Epoch @ 111 L: 1.952342 M: 0.069887 S: 0.278814  -- V2LR: Epoch M: 0.177416
Task: Training Epoch @ 114 L: 0.964237 M: 0.026336 S: 0.150133  -- V2LR: Epoch M: 0.180076 !==! Task: Validation Epoch @ 114 L: 1.904203 M: 0.067370 S: 0.287669  -- V2LR: Epoch M: 0.181216
Task: Training Epoch @ 127 L: 0.931235 M: 0.025388 S: 0.145535  -- V2LR: Epoch M: 0.186707 !==! Task: Validation Epoch @ 127 L: 1.888088 M: 0.066472 S: 0.279675  -- V2LR: Epoch M: 0.182748
Task: Training Epoch @ 141 L: 0.915706 M: 0.024908 S: 0.143291  -- V2LR: Epoch M: 0.193387 !==! Task: Validation Epoch @ 141 L: 1.874558 M: 0.065484 S: 0.288640  -- V2LR: Epoch M: 0.189541
Task: Training Epoch @ 150 L: 0.894562 M: 0.024346 S: 0.140358  -- V2LR: Epoch M: 0.198335 !==! Task: Validation Epoch @ 150 L: 1.870392 M: 0.065337 S: 0.287580  -- V2LR: Epoch M: 0.195013
Tolerance: 1!! Task: Training Epoch @ 171 L: 0.863410 M: 0.023498 S: 0.136461  -- V2LR: Epoch M: 0.208527 !==! Task: Validation Epoch @ 171 L: 1.908968 M: 0.068082 S: 0.286504  -- V2LR: Epoch M: 0.206714
Task: Testing Epoch @ -01 L: 2.341390 M: 0.089709 S: 0.363063  -- V2LR: Epoch M: 0.199128
written to: ./models/v2lr/1.5.vgg.b.1.20231221023841_v2lr.pt
written to: ./results/loss_tracker_v2lr.csv
Elapsed time: 6631.785707473755 seconds.
