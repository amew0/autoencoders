33.1
31_wz_14.1_param_init_changed
Importing finished!!
cuda is going to be used!!
Dataset loaded!!
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]             320
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3           [-1, 48, 12, 12]          38,448
              ReLU-4           [-1, 48, 12, 12]               0
            Conv2d-5             [-1, 96, 8, 8]          18,528
              ReLU-6             [-1, 96, 8, 8]               0
            Conv2d-7            [-1, 192, 6, 6]         166,080
              ReLU-8            [-1, 192, 6, 6]               0
            Conv2d-9             [-1, 12, 3, 3]          20,748
             ReLU-10             [-1, 12, 3, 3]               0
          Flatten-11                  [-1, 108]               0
        Unflatten-12             [-1, 12, 3, 3]               0
  ConvTranspose2d-13            [-1, 192, 6, 6]          20,928
  ConvTranspose2d-14             [-1, 96, 8, 8]         165,984
             ReLU-15             [-1, 96, 8, 8]               0
  ConvTranspose2d-16           [-1, 48, 12, 12]          18,480
             ReLU-17           [-1, 48, 12, 12]               0
  ConvTranspose2d-18            [-1, 1, 24, 24]             433
             Tanh-19            [-1, 1, 24, 24]               0
================================================================
Total params: 449,949
Trainable params: 244,124
Non-trainable params: 205,825
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.69
Params size (MB): 1.72
Estimated Total Size (MB): 2.41
----------------------------------------------------------------
None
Training started!!
Epoch:0, Loss:0.016032
Epoch:1, Loss:0.016032
Epoch:2, Loss:0.016032
Epoch:3, Loss:0.016031
Epoch:4, Loss:0.016031
Epoch:5, Loss:0.016031
Epoch:6, Loss:0.016031
Epoch:7, Loss:0.016032
Epoch:8, Loss:0.016032
Epoch:9, Loss:0.016031
Epoch:10, Loss:0.016032
Epoch:11, Loss:0.016032
Epoch:12, Loss:0.016032
Epoch:13, Loss:0.016032
Epoch:14, Loss:0.016031
Epoch:15, Loss:0.016032
Epoch:16, Loss:0.016032
Epoch:17, Loss:0.016032
Epoch:18, Loss:0.016032
Epoch:19, Loss:0.016032
Epoch:20, Loss:0.016032
Epoch:21, Loss:0.016031
Epoch:22, Loss:0.016032
Epoch:23, Loss:0.016032
Epoch:24, Loss:0.016032
Epoch:25, Loss:0.016031
Epoch:26, Loss:0.016032
Epoch:27, Loss:0.016032
Epoch:28, Loss:0.016032
Epoch:29, Loss:0.016032
Epoch:30, Loss:0.016032
Epoch:31, Loss:0.016032
Epoch:32, Loss:0.016032
Epoch:33, Loss:0.016032
Epoch:34, Loss:0.016032
Epoch:35, Loss:0.016032
Epoch:36, Loss:0.016032
Epoch:37, Loss:0.016032
Epoch:38, Loss:0.016032
Epoch:39, Loss:0.016032
Epoch:40, Loss:0.016032
Epoch:41, Loss:0.016032
Epoch:42, Loss:0.016032
Epoch:43, Loss:0.016032
Epoch:44, Loss:0.016032
Epoch:45, Loss:0.016032
Epoch:46, Loss:0.016032
Epoch:47, Loss:0.016032
Epoch:48, Loss:0.016032
Epoch:49, Loss:0.016032
Epoch:50, Loss:0.016032
Epoch:51, Loss:0.016032
Epoch:52, Loss:0.016032
Epoch:53, Loss:0.016032
Epoch:54, Loss:0.016032
Epoch:55, Loss:0.016032
Epoch:56, Loss:0.016032
Epoch:57, Loss:0.016032
Epoch:58, Loss:0.016032
Epoch:59, Loss:0.016032
Epoch:60, Loss:0.016032
Epoch:61, Loss:0.016032
Epoch:62, Loss:0.016032
Epoch:63, Loss:0.016032
Epoch:64, Loss:0.016032
Epoch:65, Loss:0.016032
Epoch:66, Loss:0.016032
Epoch:67, Loss:0.016032
Epoch:68, Loss:0.016032
Epoch:69, Loss:0.016032
Epoch:70, Loss:0.016032
Epoch:71, Loss:0.016032
Epoch:72, Loss:0.016032
Epoch:73, Loss:0.016032
Epoch:74, Loss:0.016032
Epoch:75, Loss:0.016032
Epoch:76, Loss:0.016032
Epoch:77, Loss:0.016032
Epoch:78, Loss:0.016032
Epoch:79, Loss:0.016032
Epoch:80, Loss:0.016032
Epoch:81, Loss:0.016032
Epoch:82, Loss:0.016032
Epoch:83, Loss:0.016032
Epoch:84, Loss:0.016032
Epoch:85, Loss:0.016032
Epoch:86, Loss:0.016032
Epoch:87, Loss:0.016032
Epoch:88, Loss:0.016032
Epoch:89, Loss:0.016032
Epoch:90, Loss:0.016032
Epoch:91, Loss:0.016032
Epoch:92, Loss:0.016032
Epoch:93, Loss:0.016032
Epoch:94, Loss:0.016032
Epoch:95, Loss:0.016032
Epoch:96, Loss:0.016032
Epoch:97, Loss:0.016032
Epoch:98, Loss:0.016032
Epoch:99, Loss:0.016032
Epoch:100, Loss:0.016032
Epoch:101, Loss:0.016032
Epoch:102, Loss:0.016032
Epoch:103, Loss:0.016032
Epoch:104, Loss:0.016032
Epoch:105, Loss:0.016032
Epoch:106, Loss:0.016032
Epoch:107, Loss:0.016032
Epoch:108, Loss:0.016032
Epoch:109, Loss:0.016032
Epoch:110, Loss:0.016032
Epoch:111, Loss:0.016032
Epoch:112, Loss:0.016032
Epoch:113, Loss:0.016032
Epoch:114, Loss:0.016032
Epoch:115, Loss:0.016032
Epoch:116, Loss:0.016032
Epoch:117, Loss:0.016032
Epoch:118, Loss:0.016032
Epoch:119, Loss:0.016032
Epoch:120, Loss:0.016032
Epoch:121, Loss:0.016032
Epoch:122, Loss:0.016032
Epoch:123, Loss:0.016032
Epoch:124, Loss:0.016032
Epoch:125, Loss:0.016032
Epoch:126, Loss:0.016032
Epoch:127, Loss:0.016032
Epoch:128, Loss:0.016032
Epoch:129, Loss:0.016032
Epoch:130, Loss:0.016032
Epoch:131, Loss:0.016032
Epoch:132, Loss:0.016032
Epoch:133, Loss:0.016032
Epoch:134, Loss:0.016032
Epoch:135, Loss:0.016032
Epoch:136, Loss:0.016032
Epoch:137, Loss:0.016032
Epoch:138, Loss:0.016032
Epoch:139, Loss:0.016032
Epoch:140, Loss:0.016032
Epoch:141, Loss:0.016032
Epoch:142, Loss:0.016032
Epoch:143, Loss:0.016032
Epoch:144, Loss:0.016032
Epoch:145, Loss:0.016032
Epoch:146, Loss:0.016032
Epoch:147, Loss:0.016032
Epoch:148, Loss:0.016032
Epoch:149, Loss:0.016032
Epoch:150, Loss:0.016032
Epoch:151, Loss:0.016032
Epoch:152, Loss:0.016032
Epoch:153, Loss:0.016032
Epoch:154, Loss:0.016032
Epoch:155, Loss:0.016032
Epoch:156, Loss:0.016032
Epoch:157, Loss:0.016032
Epoch:158, Loss:0.016032
Epoch:159, Loss:0.016032
Epoch:160, Loss:0.016032
Epoch:161, Loss:0.016032
Epoch:162, Loss:0.016032
Epoch:163, Loss:0.016032
Epoch:164, Loss:0.016032
Epoch:165, Loss:0.016032
Epoch:166, Loss:0.016032
Epoch:167, Loss:0.016032
Epoch:168, Loss:0.016032
Epoch:169, Loss:0.016032
Epoch:170, Loss:0.016032
Epoch:171, Loss:0.016032
Epoch:172, Loss:0.016032
Epoch:173, Loss:0.016032
Epoch:174, Loss:0.016032
Epoch:175, Loss:0.016032
Epoch:176, Loss:0.016032
Epoch:177, Loss:0.016032
Epoch:178, Loss:0.016032
Epoch:179, Loss:0.016032
Epoch:180, Loss:0.016032
Epoch:181, Loss:0.016032
Epoch:182, Loss:0.016032
Epoch:183, Loss:0.016032
Epoch:184, Loss:0.016032
Epoch:185, Loss:0.016032
Epoch:186, Loss:0.016032
Epoch:187, Loss:0.016032
Epoch:188, Loss:0.016032
Epoch:189, Loss:0.016032
Epoch:190, Loss:0.016032
Epoch:191, Loss:0.016032
Epoch:192, Loss:0.016032
Epoch:193, Loss:0.016032
Epoch:194, Loss:0.016032
Epoch:195, Loss:0.016032
Epoch:196, Loss:0.016032
Epoch:197, Loss:0.016032
Epoch:198, Loss:0.016032
Epoch:199, Loss:0.016032
Avg Test Loss: 0.014292954602201159
written to: ./results/loss_tracker_diffimg.csv
written to: ./models/33.1.20231109215901_diffimg.pth
written to: ./models/33.1.20231109215901_diffimg.pt
Elapsed time: 1681.5216836929321 seconds.
